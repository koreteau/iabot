import gradio as gr
import requests
import json
import time
import re
import os
import glob


F1_FOLDERS = {
    "calendrier": "data/calendrier",
    "circuits": "data/circuits",
    "classements": "data/classement"
}

F1_KEYWORDS = ["F1", "Formula 1", "Grand Prix", "pilote", "√©curie", "championnat", "circuit", "calendrier", "classement", "course", "GP"]

def load_f1_data():
    """ Charge toutes les donn√©es F1 des fichiers Markdown en m√©moire sous forme de dictionnaire """
    f1_data = {}
    
    for category, folder in F1_FOLDERS.items():
        if not os.path.exists(folder):
            print(f"üö´ Dossier introuvable : {folder}")
            continue

        for filepath in glob.glob(os.path.join(folder, "*.md")):
            with open(filepath, "r", encoding="utf-8") as file:
                content = file.read()
                year_match = re.search(r"(\d{4})", filepath)
                if year_match:
                    year = int(year_match.group(1))
                    if year not in f1_data:
                        f1_data[year] = {}
                    f1_data[year][category] = content
                    print(f"‚úÖ Donn√©es charg√©es pour {year} ({category}) depuis {filepath}")

    return f1_data

F1_DATABASE = load_f1_data()

def is_f1_question(user_input):
    """ V√©rifie si la question concerne la F1 en cherchant des mots-cl√©s """
    return any(keyword.lower() in user_input.lower() for keyword in F1_KEYWORDS)

def extract_latest_data():
    """ Construit un contexte avec les informations F1 les plus r√©centes des fichiers `.md` """
    latest_year = max(F1_DATABASE.keys(), default=None)
    if not latest_year:
        return None 

    latest_data = []
    for category, content in F1_DATABASE[latest_year].items():
        latest_data.append(f"### {category.capitalize()} - {latest_year}\n{content}")

    return "\n\n".join(latest_data) if latest_data else None

def query_ollama(user_input, f1_context):
    """ Envoie une requ√™te √† DeepSeek en lui imposant d'utiliser les fichiers `.md` """
    
    if f1_context:
        prompt = f"""
        Tu es un expert en Formule 1. Voici les donn√©es les plus r√©centes extraites de fichiers officiels :
        
        {f1_context}
        
        Utilise **uniquement ces informations** pour g√©n√©rer ta r√©ponse.
        Ne t'appuie PAS sur tes propres connaissances si la r√©ponse est dans les fichiers fournis.

        Question de l'utilisateur :
        {user_input}
        """
    else:
        prompt = f"""
        Tu es un expert en Formule 1. Aucune donn√©e locale r√©cente n'est disponible.
        R√©ponds avec tes propres connaissances, mais pr√©cise bien que tes informations peuvent √™tre obsol√®tes.

        Question de l'utilisateur :
        {user_input}
        """
    
    try:
        res = requests.post("http://localhost:11434/api/generate", json={
            "model": "deepseek-r1:14b",
            "prompt": prompt
        }, stream=True)

        bot_response = ""
        for line in res.iter_lines():
            if line:
                try:
                    data = json.loads(line.decode("utf-8"))
                    bot_response += data.get("response", "")
                except json.JSONDecodeError:
                    continue

        return bot_response.strip() or "‚ö†Ô∏è Je n'ai pas compris ta question."
    
    except requests.exceptions.ConnectionError:
        return "‚ö†Ô∏è Erreur de connexion avec Ollama. V√©rifie qu'il tourne avec `ollama serve`."

def respond(user_input, chat_history):
    """ G√©n√®re une r√©ponse combin√©e entre DeepSeek et nos fichiers F1 """

    # V√©rifier si la question concerne la F1
    f1_data = extract_latest_data() if is_f1_question(user_input) else None
    
    # Demander une r√©ponse √† DeepSeek en lui imposant nos fichiers comme source principale
    ollama_response = query_ollama(user_input, f1_data)

    # Comparer les r√©sultats et choisir la meilleure r√©ponse
    response = ollama_response

    # Nettoyage du texte pour enlever les raisonnements de l'ia
    response = re.sub(r"<think>.*?</think>", "", response, flags=re.DOTALL).strip()

    # Mise √† jour de l'historique
    chat_history.append({"role": "user", "content": user_input})
    chat_history.append({"role": "assistant", "content": response})

    print("R√©ponse g√©n√©r√©e :", response)

    return "", chat_history


with gr.Blocks() as demo:
    gr.Markdown("# üèéÔ∏è APEXBOT")

    chatbot = gr.Chatbot(type="messages")
    msg = gr.Textbox(label="Pose ta question", placeholder="Ex: Qui a gagn√© le Grand Prix de Monza en 2020 ?")
    clear = gr.ClearButton([msg, chatbot])

    msg.submit(respond, [msg, chatbot], [msg, chatbot])

if __name__ == "__main__":
    demo.launch()