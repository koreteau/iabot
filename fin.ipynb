{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "exception: access violation reading 0x0000000000000000",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mllama_cpp\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Llama  \u001b[38;5;66;03m# üöÄ Mod√®le LLM local\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# üìå Charger le mod√®le LLM (Mistral 7B GGUF)\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mLlama\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmistral-7b-instruct-v0.1.Q4_K_M.gguf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_ctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2048\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\cantfly\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\llama_cpp\\llama.py:204\u001b[0m, in \u001b[0;36mLlama.__init__\u001b[1;34m(self, model_path, n_gpu_layers, split_mode, main_gpu, tensor_split, rpc_servers, vocab_only, use_mmap, use_mlock, kv_overrides, seed, n_ctx, n_batch, n_ubatch, n_threads, n_threads_batch, rope_scaling_type, pooling_type, rope_freq_base, rope_freq_scale, yarn_ext_factor, yarn_attn_factor, yarn_beta_fast, yarn_beta_slow, yarn_orig_ctx, logits_all, embedding, offload_kqv, flash_attn, no_perf, last_n_tokens_size, lora_base, lora_scale, lora_path, numa, chat_format, chat_handler, draft_model, tokenizer, type_k, type_v, spm_infill, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Llama\u001b[38;5;241m.\u001b[39m__backend_initialized:\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m suppress_stdout_stderr(disable\u001b[38;5;241m=\u001b[39mverbose):\n\u001b[1;32m--> 204\u001b[0m         \u001b[43mllama_cpp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllama_backend_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    205\u001b[0m     Llama\u001b[38;5;241m.\u001b[39m__backend_initialized \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(numa, \u001b[38;5;28mbool\u001b[39m):\n",
      "\u001b[1;31mOSError\u001b[0m: exception: access violation reading 0x0000000000000000"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from llama_cpp import Llama  # üöÄ Mod√®le LLM local\n",
    "\n",
    "# üìå Charger le mod√®le LLM (Mistral 7B GGUF)\n",
    "llm = Llama(model_path=\"mistral-7b-instruct-v0.1.Q4_K_M.gguf\", n_ctx=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# üìå Fonction pour extraire l'ann√©e d'un nom de fichier\n",
    "def extract_year_from_filename(filename):\n",
    "    match = re.search(r\"(20\\d{2})\", filename)\n",
    "    return int(match.group(1)) if match else None\n",
    "\n",
    "# üìå Dossier contenant les fichiers CSV\n",
    "data_folder = \"data\"\n",
    "\n",
    "def search_f1_data(question):\n",
    "    question = question.lower()\n",
    "    files_to_return = []  # Liste des fichiers CSV √† retourner\n",
    "\n",
    "    # üìå V√©rifier s'il y a une ann√©e sp√©cifique dans la question\n",
    "    match = re.search(r\"(20\\d{2})\", question)\n",
    "    year = int(match.group(1)) if match else None\n",
    "\n",
    "    # üìå V√©rifier si l'utilisateur demande les 5 derni√®res ann√©es\n",
    "    if any(keyword in question for keyword in [\"5 derni√®res ann√©es\", \"derni√®res ann√©es\", \"cinq ans\",\"l'ann√©e prochaine\"]):\n",
    "        if year:\n",
    "            start_year = max(2020, year - 4)  # Assurer qu'on ne descend pas sous 2020\n",
    "        else:\n",
    "            start_year = 2020  # Par d√©faut, on prend les 5 derni√®res ann√©es disponibles\n",
    "        years = list(range(start_year, start_year + 5))\n",
    "        print(\"5 derni√®res ann√©es:\", years)\n",
    "    else:\n",
    "        years = [year] if year else []\n",
    "        print(\"ann√©e:\", years)\n",
    "\n",
    "    # üìå Identifier les fichiers √† r√©cup√©rer\n",
    "    if \"classement\" or \"champion\" or \"pilote\" in question:\n",
    "        print(\"classement\")\n",
    "        files_to_return = [f\"classement/classement_f1_{y}.csv\" for y in years]\n",
    "        print(files_to_return)\n",
    "    elif \"calendrier\" or \"date\" in question:\n",
    "        files_to_return = [f\"calendrier/calendrier_f1_{y}.csv\" for y in years]\n",
    "    elif \"circuit\" in question:\n",
    "        files_to_return = [f\"circuits/circuits_f1_{y}.csv\" for y in years]\n",
    "    elif \"constructeur\" in question:\n",
    "        files_to_return = [f\"constructeurs/constructeurs_f1_{y}.csv\" for y in years]\n",
    "# üìå Charger et concat√©ner les fichiers trouv√©s\n",
    "    data_results = []\n",
    "    for file_key in files_to_return:\n",
    "        file_path = os.path.join(data_folder, file_key)\n",
    "        print(\"Fichier:\", file_path)\n",
    "        if os.path.exists(file_path):\n",
    "            df = pd.read_csv(file_path)\n",
    "            data_results.append(f\"üìÇ **Donn√©es de {file_key}**\\n{df.to_string(index=False)}\\n\\n\")\n",
    "\n",
    "    # üìå Retourner les r√©sultats concat√©n√©s\n",
    "    if data_results:\n",
    "        \n",
    "        return \"\\n\".join(data_results)\n",
    "\n",
    "    return \"D√©sol√©, je n'ai pas trouv√© de donn√©es correspondantes.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(question, data_results):\n",
    "    # üî• Limite la taille des donn√©es envoy√©es\n",
    "    if len(data_results) > 1500:\n",
    "        data_results = data_results[:1500]  # Garde uniquement les premiers 1500 caract√®res\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    [INST] \n",
    "    Tu es un expert en Formule 1. Un utilisateur te pose la question :\n",
    "    \"{question}\"\n",
    "\n",
    "    Voici les donn√©es brutes extraites des fichiers CSV :\n",
    "    {data_results}\n",
    "\n",
    "    Analyse les donn√©es et donne une r√©ponse claire et concise.\n",
    "    [/INST]\n",
    "    \"\"\"\n",
    "\n",
    "    response = llm(prompt, max_tokens=256, stream=True)\n",
    "    for chunk in response:\n",
    "        print(chunk[\"choices\"][0][\"text\"], end=\"\", flush=True)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "while True:\n",
    "        user_input = input(\"Pose une question sur la F1 : \")\n",
    "        if user_input.lower() == \"exit\":\n",
    "            break\n",
    "\n",
    "        data_results = search_f1_data(user_input)\n",
    "        generate_response(user_input, data_results)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
