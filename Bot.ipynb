{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\cantfly\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\cantfly\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\cantfly\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatterBot installÃ© avec succÃ¨s !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    }
   ],
   "source": [
    "from chatterbot import ChatBot\n",
    "chatbot = ChatBot(\"TestBot\")\n",
    "print(\"ChatterBot installÃ© avec succÃ¨s !\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training botprofile.yml: [#                   ] 5%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\cantfly\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\cantfly\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training botprofile.yml: [####################] 100%\n",
      "Training conversations.yml: [####################] 100%\n",
      "Training food.yml: [####################] 100%\n",
      "Training greetings.yml: [####################] 100%\n",
      "Training trivia.yml: [####################] 100%\n",
      "Tape 'exit' pour quitter.\n",
      "Bot: Bonjour, Comment ca va?\n",
      "Bot: Je consomme de la RAM, et des chiffres binaires.\n",
      "Bot: Les Royaumes-Unis de Grande Bretagne\n",
      "Bot: Salut\n",
      "Bot: Je suis assez jeune selon vos standards.\n",
      "Bot: Sputnik 1\n",
      "Bot: D'apres quel astronaute americain, le telescope spatial Hubble, lance en 1990 et place sur orbite basse, est-il nomme?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No value for search_text was available on the provided input\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: Quels sont tes centres d'interets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No value for search_text was available on the provided input\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: Quels sont tes centres d'interets\n"
     ]
    }
   ],
   "source": [
    "from chatterbot import ChatBot\n",
    "from chatterbot.trainers import ChatterBotCorpusTrainer\n",
    "\n",
    "# CrÃ©ation du chatbot\n",
    "chatbot = ChatBot(\"SimpleBot\")\n",
    "\n",
    "# EntraÃ®nement avec des donnÃ©es de base\n",
    "trainer = ChatterBotCorpusTrainer(chatbot)\n",
    "trainer.train(\"chatterbot.corpus.french\")  # EntraÃ®nement avec le corpus franÃ§ais\n",
    "\n",
    "# Boucle de discussion\n",
    "print(\"Tape 'exit' pour quitter.\")\n",
    "while True:\n",
    "    user_input = input(\"Vous: \")\n",
    "    if user_input.lower() == \"exit\":\n",
    "        break\n",
    "    response = chatbot.get_response(user_input)\n",
    "    print(\"Bot:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\cantfly\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\cantfly\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\cantfly\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\cantfly\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\cantfly\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List Trainer: [                    ] 1%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\cantfly\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\cantfly\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List Trainer: [####################] 100%\n",
      "âœ… Chatbot F1 entraÃ®nÃ© avec succÃ¨s !\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\cantfly/nltk_data'\n    - 'c:\\\\Users\\\\cantfly\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python38\\\\nltk_data'\n    - 'c:\\\\Users\\\\cantfly\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python38\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\cantfly\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python38\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\cantfly\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 118\u001b[0m\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;66;03m# ğŸ“Œ VÃ©rifier si la rÃ©ponse doit Ãªtre recherchÃ©e dans les fichiers CSV\u001b[39;00m\n\u001b[1;32m--> 118\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43msearch_f1_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDÃ©solÃ©, je n\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mai pas cette information.\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    120\u001b[0m     response \u001b[38;5;241m=\u001b[39m chatbot\u001b[38;5;241m.\u001b[39mget_response(user_input)  \u001b[38;5;66;03m# Demander Ã  ChatterBot\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[14], line 87\u001b[0m, in \u001b[0;36msearch_f1_data\u001b[1;34m(question)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msearch_f1_data\u001b[39m(question):\n\u001b[1;32m---> 87\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m \u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Tokeniser la question\u001b[39;00m\n\u001b[0;32m     88\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDÃ©solÃ©, je n\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mai pas cette information.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     90\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m file, df \u001b[38;5;129;01min\u001b[39;00m csv_files\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[1;32mc:\\Users\\cantfly\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\nltk\\tokenize\\__init__.py:142\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 142\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    144\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[0;32m    145\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\cantfly\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32mc:\\Users\\cantfly\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[1;34m(language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get_punkt_tokenizer\u001b[39m(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;124;03m    a lru cache for performance.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03m    :type language: str\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\cantfly\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m-> 1744\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\cantfly\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1747\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[1;32m-> 1749\u001b[0m     lang_dir \u001b[38;5;241m=\u001b[39m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[0;32m   1751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang \u001b[38;5;241m=\u001b[39m lang\n",
      "File \u001b[1;32mc:\\Users\\cantfly\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\nltk\\data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\cantfly/nltk_data'\n    - 'c:\\\\Users\\\\cantfly\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python38\\\\nltk_data'\n    - 'c:\\\\Users\\\\cantfly\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python38\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\cantfly\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python38\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\cantfly\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from chatterbot import ChatBot\n",
    "from chatterbot.trainers import ListTrainer\n",
    "import nltk\n",
    "\n",
    "# Forcer le tÃ©lÃ©chargement des ressources nÃ©cessaires\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "\n",
    "# ğŸ“Œ Initialisation du chatbot\n",
    "chatbot = ChatBot(\"F1Bot\")\n",
    "\n",
    "# ğŸ“Œ Dossier contenant les fichiers CSV\n",
    "data_folder = \"data\"  # ğŸ› ï¸ Change ce chemin selon ton projet\n",
    "\n",
    "# ğŸ“Œ Chargement automatique des fichiers CSV depuis les sous-dossiers\n",
    "csv_files = {}\n",
    "\n",
    "for root, _, files in os.walk(data_folder):  # ğŸ” Recherche dans tous les sous-dossiers\n",
    "    for file in files:\n",
    "        if file.endswith(\".csv\"):\n",
    "            file_path = os.path.join(root, file)\n",
    "            csv_files[file] = pd.read_csv(file_path)\n",
    "\n",
    "# ğŸ“Œ Liste pour stocker les questions-rÃ©ponses de base\n",
    "training_data = []\n",
    "\n",
    "# ğŸ“… **1ï¸âƒ£ GÃ©nÃ©rer des questions-rÃ©ponses basÃ©es sur les CSV**\n",
    "for file, df in csv_files.items():\n",
    "    try:\n",
    "        year = next(filter(str.isdigit, file.split(\"_\")), \"inconnue\")\n",
    "    except StopIteration:\n",
    "        year = \"inconnue\"\n",
    "\n",
    "    # ğŸ¯ EntraÃ®nement basÃ© sur le calendrier des courses\n",
    "    if \"calendrier\" in file.lower():\n",
    "        for _, row in df.iterrows():\n",
    "            gp = row.get(\"Grand Prix\", \"Inconnu\")\n",
    "            circuit = row.get(\"Circuit\", \"Inconnu\")\n",
    "            ville = row.get(\"Ville\", \"Inconnu\")\n",
    "            pays = row.get(\"Pays\", \"Inconnu\")\n",
    "            date = row.get(\"Date\", \"Inconnu\")\n",
    "            heure = row.get(\"Heure\", \"Inconnu\")\n",
    "\n",
    "            training_data.append(f\"Quand a eu lieu le {gp} en {year} ?\")\n",
    "            training_data.append(f\"Le {gp} en {year} a eu lieu le {date} Ã  {ville}, {pays}.\")\n",
    "\n",
    "            training_data.append(f\"OÃ¹ s'est dÃ©roulÃ© le {gp} en {year} ?\")\n",
    "            training_data.append(f\"Le {gp} en {year} s'est dÃ©roulÃ© sur le circuit {circuit}, situÃ© Ã  {ville}, {pays}.\")\n",
    "\n",
    "            training_data.append(f\"Ã€ quelle heure a commencÃ© le {gp} en {year} ?\")\n",
    "            training_data.append(f\"Le {gp} en {year} a commencÃ© Ã  {heure}.\")\n",
    "\n",
    "    # ğŸ† EntraÃ®nement basÃ© sur les classements\n",
    "    if \"classement\" in file.lower():\n",
    "        for i, row in df.iterrows():\n",
    "            pilote = row.get(\"Pilote\", \"Inconnu\")\n",
    "            equipe = row.get(\"Ã‰quipe\", \"Inconnu\")\n",
    "            points = row.get(\"Points\", \"Inconnu\")\n",
    "\n",
    "            if i == 0:  # Premier pilote = Champion du monde\n",
    "                training_data.append(f\"Qui a gagnÃ© le championnat en {year} ?\")\n",
    "                training_data.append(f\"Le champion du monde en {year} Ã©tait {pilote} avec {points} points.\")\n",
    "\n",
    "            training_data.append(f\"Combien de points a eu {pilote} en {year} ?\")\n",
    "            training_data.append(f\"{pilote} a obtenu {points} points en {year}.\")\n",
    "\n",
    "            training_data.append(f\"Dans quelle Ã©quipe Ã©tait {pilote} en {year} ?\")\n",
    "            training_data.append(f\"{pilote} courait pour {equipe} en {year}.\")\n",
    "\n",
    "# ğŸ“Œ EntraÃ®nement du chatbot\n",
    "trainer = ListTrainer(chatbot)\n",
    "trainer.train(training_data)\n",
    "\n",
    "print(\"âœ… Chatbot F1 entraÃ®nÃ© avec succÃ¨s !\")\n",
    "\n",
    "\n",
    "# ğŸ“Œ **2ï¸âƒ£ Moteur de recherche intelligent**\n",
    "def search_f1_data(question):\n",
    "    tokens = word_tokenize(question.lower())  # Tokeniser la question\n",
    "    result = \"DÃ©solÃ©, je n'ai pas cette information.\"\n",
    "\n",
    "    for file, df in csv_files.items():\n",
    "        year = next(filter(str.isdigit, file.split(\"_\")), \"inconnue\")\n",
    "\n",
    "        if \"calendrier\" in file.lower() and (\"quand\" in tokens or \"date\" in tokens):\n",
    "            for _, row in df.iterrows():\n",
    "                if any(word in row.get(\"Grand Prix\", \"\").lower() for word in tokens):\n",
    "                    return f\"Le {row['Grand Prix']} a eu lieu le {row['Date']} Ã  {row['Ville']}, {row['Pays']}.\"\n",
    "\n",
    "        if \"classement\" in file.lower() and \"points\" in tokens:\n",
    "            for _, row in df.iterrows():\n",
    "                if any(word in row.get(\"Pilote\", \"\").lower() for word in tokens):\n",
    "                    return f\"{row['Pilote']} a obtenu {row['Points']} points en {year}.\"\n",
    "\n",
    "        if \"constructeurs\" in file.lower() and \"constructeur\" in tokens:\n",
    "            for _, row in df.iterrows():\n",
    "                if any(word in row.get(\"Ã‰quipe\", \"\").lower() for word in tokens):\n",
    "                    return f\"L'Ã©quipe {row['Ã‰quipe']} a terminÃ© avec {row['Points']} points en {year}.\"\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# ğŸ“Œ **3ï¸âƒ£ Lancer le chatbot interactif**\n",
    "while True:\n",
    "    user_input = input(\"Pose une question sur la F1 : \")\n",
    "    if user_input.lower() == \"exit\":\n",
    "        break\n",
    "\n",
    "    # ğŸ“Œ VÃ©rifier si la rÃ©ponse doit Ãªtre recherchÃ©e dans les fichiers CSV\n",
    "    response = search_f1_data(user_input)\n",
    "    if response == \"DÃ©solÃ©, je n'ai pas cette information.\":\n",
    "        response = chatbot.get_response(user_input)  # Demander Ã  ChatterBot\n",
    "\n",
    "    print(\"F1Bot:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ Erreur lors de la tokenisation : \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\cantfly/nltk_data'\n",
      "    - 'c:\\\\Users\\\\cantfly\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python38\\\\nltk_data'\n",
      "    - 'c:\\\\Users\\\\cantfly\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python38\\\\share\\\\nltk_data'\n",
      "    - 'c:\\\\Users\\\\cantfly\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python38\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\cantfly\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\cantfly\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')  # ğŸ“Œ Assure que punkt est bien tÃ©lÃ©chargÃ©\n",
    "\n",
    "# Test de la tokenisation\n",
    "question = \"Quand a eu lieu le Grand Prix de Monaco ?\"\n",
    "try:\n",
    "    tokens = word_tokenize(question.lower())\n",
    "    print(\"âœ… Tokenisation rÃ©ussie :\", tokens)\n",
    "except Exception as e:\n",
    "    print(\"âŒ Erreur lors de la tokenisation :\", str(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ChargÃ© : calendrier_f1_2020.csv\n",
      "âœ… ChargÃ© : calendrier_f1_2021.csv\n",
      "âœ… ChargÃ© : calendrier_f1_2022.csv\n",
      "âœ… ChargÃ© : calendrier_f1_2023.csv\n",
      "âœ… ChargÃ© : calendrier_f1_2024.csv\n",
      "âœ… ChargÃ© : circuits_f1_2020.csv\n",
      "âœ… ChargÃ© : circuits_f1_2021.csv\n",
      "âœ… ChargÃ© : circuits_f1_2022.csv\n",
      "âœ… ChargÃ© : circuits_f1_2023.csv\n",
      "âœ… ChargÃ© : circuits_f1_2024.csv\n",
      "âœ… ChargÃ© : classement_f1_2020.csv\n",
      "âœ… ChargÃ© : classement_f1_2021.csv\n",
      "âœ… ChargÃ© : classement_f1_2022.csv\n",
      "âœ… ChargÃ© : classement_f1_2023.csv\n",
      "âœ… ChargÃ© : classement_f1_2024.csv\n",
      "âœ… ChargÃ© : constructeurs_f1_2020.csv\n",
      "âœ… ChargÃ© : constructeurs_f1_2021.csv\n",
      "âœ… ChargÃ© : constructeurs_f1_2022.csv\n",
      "âœ… ChargÃ© : constructeurs_f1_2023.csv\n",
      "âœ… ChargÃ© : constructeurs_f1_2024.csv\n",
      "ğŸ“‚ calendrier_f1_2020.csv : 17 lignes, 7 colonnes\n",
      "ğŸ“‚ calendrier_f1_2021.csv : 22 lignes, 7 colonnes\n",
      "ğŸ“‚ calendrier_f1_2022.csv : 22 lignes, 7 colonnes\n",
      "ğŸ“‚ calendrier_f1_2023.csv : 22 lignes, 7 colonnes\n",
      "ğŸ“‚ calendrier_f1_2024.csv : 24 lignes, 7 colonnes\n",
      "ğŸ“‚ circuits_f1_2020.csv : 14 lignes, 4 colonnes\n",
      "ğŸ“‚ circuits_f1_2021.csv : 21 lignes, 4 colonnes\n",
      "ğŸ“‚ circuits_f1_2022.csv : 22 lignes, 4 colonnes\n",
      "ğŸ“‚ circuits_f1_2023.csv : 22 lignes, 4 colonnes\n",
      "ğŸ“‚ circuits_f1_2024.csv : 24 lignes, 4 colonnes\n",
      "ğŸ“‚ classement_f1_2020.csv : 23 lignes, 5 colonnes\n",
      "ğŸ“‚ classement_f1_2021.csv : 21 lignes, 5 colonnes\n",
      "ğŸ“‚ classement_f1_2022.csv : 22 lignes, 5 colonnes\n",
      "ğŸ“‚ classement_f1_2023.csv : 22 lignes, 5 colonnes\n",
      "ğŸ“‚ classement_f1_2024.csv : 24 lignes, 5 colonnes\n",
      "ğŸ“‚ constructeurs_f1_2020.csv : 10 lignes, 4 colonnes\n",
      "ğŸ“‚ constructeurs_f1_2021.csv : 10 lignes, 4 colonnes\n",
      "ğŸ“‚ constructeurs_f1_2022.csv : 10 lignes, 4 colonnes\n",
      "ğŸ“‚ constructeurs_f1_2023.csv : 10 lignes, 4 colonnes\n",
      "ğŸ“‚ constructeurs_f1_2024.csv : 10 lignes, 4 colonnes\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "data_folder = \"data\"  # ğŸ› ï¸ Mets ton chemin ici\n",
    "\n",
    "# VÃ©rifier si les fichiers sont trouvÃ©s\n",
    "csv_files = {}\n",
    "\n",
    "for root, _, files in os.walk(data_folder):  \n",
    "    for file in files:\n",
    "        if file.endswith(\".csv\"):\n",
    "            file_path = os.path.join(root, file)\n",
    "            try:\n",
    "                df = pd.read_csv(file_path)\n",
    "                csv_files[file] = df\n",
    "                print(f\"âœ… ChargÃ© : {file}\")\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ Erreur de chargement {file} :\", str(e))\n",
    "\n",
    "# VÃ©rifier le contenu des fichiers chargÃ©s\n",
    "for file, df in csv_files.items():\n",
    "    print(f\"ğŸ“‚ {file} : {df.shape[0]} lignes, {df.shape[1]} colonnes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… TÃ©lÃ©chargement terminÃ©, essayez Ã  nouveau.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\cantfly\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\cantfly\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\cantfly\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\cantfly\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\cantfly\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import shutil\n",
    "\n",
    "# Supprimer tous les fichiers NLTK (si existants)\n",
    "nltk_data_path = os.path.expanduser('~/nltk_data')\n",
    "shutil.rmtree(nltk_data_path, ignore_errors=True)\n",
    "\n",
    "# TÃ©lÃ©charger Ã  nouveau les ressources nÃ©cessaires\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "print(\"âœ… TÃ©lÃ©chargement terminÃ©, essayez Ã  nouveau.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\cantfly\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\cantfly/nltk_data'\n    - 'c:\\\\Users\\\\cantfly\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python38\\\\nltk_data'\n    - 'c:\\\\Users\\\\cantfly\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python38\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\cantfly\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python38\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\cantfly\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Test de tokenisation\u001b[39;00m\n\u001b[0;32m      7\u001b[0m question \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuand a eu lieu le Grand Prix de Monaco ?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 8\u001b[0m tokens \u001b[38;5;241m=\u001b[39m \u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâœ… Tokenisation rÃ©ussie :\u001b[39m\u001b[38;5;124m\"\u001b[39m, tokens)\n",
      "File \u001b[1;32mc:\\Users\\cantfly\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\nltk\\tokenize\\__init__.py:142\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 142\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    144\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[0;32m    145\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\cantfly\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32mc:\\Users\\cantfly\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[1;34m(language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get_punkt_tokenizer\u001b[39m(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;124;03m    a lru cache for performance.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03m    :type language: str\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\cantfly\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m-> 1744\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\cantfly\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1747\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[1;32m-> 1749\u001b[0m     lang_dir \u001b[38;5;241m=\u001b[39m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[0;32m   1751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang \u001b[38;5;241m=\u001b[39m lang\n",
      "File \u001b[1;32mc:\\Users\\cantfly\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\nltk\\data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\cantfly/nltk_data'\n    - 'c:\\\\Users\\\\cantfly\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python38\\\\nltk_data'\n    - 'c:\\\\Users\\\\cantfly\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python38\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\cantfly\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python38\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\cantfly\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Test de tokenisation\n",
    "question = \"Quand a eu lieu le Grand Prix de Monaco ?\"\n",
    "tokens = word_tokenize(question.lower())\n",
    "print(\"âœ… Tokenisation rÃ©ussie :\", tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ ChargÃ© : classement F1 2020 â†’ 23 lignes, 5 colonnes\n",
      "ğŸ“‚ ChargÃ© : classement F1 2021 â†’ 21 lignes, 5 colonnes\n",
      "ğŸ“‚ ChargÃ© : classement F1 2022 â†’ 22 lignes, 5 colonnes\n",
      "ğŸ“‚ ChargÃ© : classement F1 2023 â†’ 22 lignes, 5 colonnes\n",
      "ğŸ“‚ ChargÃ© : classement F1 2024 â†’ 24 lignes, 5 colonnes\n",
      "F1Bot: Je pense que Max Verstappen sera encore en tÃªte l'annÃ©e prochaine avec environ 368 points.\n",
      "F1Bot: Je pense que Max Verstappen sera encore en tÃªte l'annÃ©e prochaine avec environ 368 points.\n",
      "F1Bot: DÃ©solÃ©, je n'ai pas cette information.\n",
      "F1Bot: Le champion du monde en 2020 Ã©tait Lewis Hamilton avec 347 points.\n",
      "F1Bot: DÃ©solÃ©, je n'ai pas cette information.\n",
      "F1Bot: DÃ©solÃ©, je n'ai pas cette information.\n",
      "F1Bot: DÃ©solÃ©, je n'ai pas cette information.\n",
      "F1Bot: DÃ©solÃ©, je n'ai pas cette information.\n",
      "F1Bot: DÃ©solÃ©, je n'ai pas cette information.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# ğŸ“Œ Fonction pour extraire l'annÃ©e du nom du fichier\n",
    "def extract_year_from_filename(filename):\n",
    "    match = re.search(r\"(20\\d{2})\", filename)  # Recherche une annÃ©e valide (ex: 2023)\n",
    "    return match.group(1) if match else \"inconnue\"\n",
    "\n",
    "# ğŸ“Œ Dossier contenant les fichiers CSV\n",
    "data_folder = \"data\"  # Mets le bon chemin ici\n",
    "\n",
    "# ğŸ“Œ Chargement automatique des fichiers de classement uniquement\n",
    "classement_files = {}\n",
    "\n",
    "for root, _, files in os.walk(data_folder):  \n",
    "    for file in files:\n",
    "        if file.startswith(\"classement_f1_\") and file.endswith(\".csv\"):\n",
    "            file_path = os.path.join(root, file)\n",
    "            year = extract_year_from_filename(file)\n",
    "            classement_files[year] = pd.read_csv(file_path)\n",
    "\n",
    "# ğŸ“Œ VÃ©rifier si les fichiers ont bien Ã©tÃ© chargÃ©s\n",
    "for year, df in classement_files.items():\n",
    "    print(f\"ğŸ“‚ ChargÃ© : classement F1 {year} â†’ {df.shape[0]} lignes, {df.shape[1]} colonnes\")\n",
    "\n",
    "# ğŸ“Œ **Moteur de recherche avancÃ©**\n",
    "def search_f1_data(question):\n",
    "    tokens = question.lower().split()\n",
    "\n",
    "    # ğŸ“Œ Recherche du champion d'une annÃ©e spÃ©cifique\n",
    "    for year in classement_files.keys():\n",
    "        if str(year) in tokens and \"champion\" in tokens:\n",
    "            df = classement_files[year]\n",
    "            champion = df.iloc[0]  # ğŸ“Œ Le premier pilote est le champion\n",
    "            return f\"Le champion du monde en {year} Ã©tait {champion['Pilote']} avec {champion['Points']} points.\"\n",
    "\n",
    "    # ğŸ“Š ğŸ“Œ Qui a eu le plus de points en 5 ans ?\n",
    "    if \"plus\" in tokens and \"points\" in tokens and \"ans\" in tokens:\n",
    "        top_pilote = None\n",
    "        max_points = 0\n",
    "        for year, df in classement_files.items():\n",
    "            total_points = df.iloc[0][\"Points\"]  # ğŸ“Œ Le champion de chaque annÃ©e\n",
    "            if total_points > max_points:\n",
    "                max_points = total_points\n",
    "                top_pilote = df.iloc[0][\"Pilote\"]\n",
    "\n",
    "        return f\"Le pilote qui a eu le plus de points ces 5 derniÃ¨res annÃ©es est {top_pilote} avec {max_points} points.\"\n",
    "\n",
    "    # ğŸ”® ğŸ“Œ PrÃ©diction du champion de l'annÃ©e prochaine\n",
    "    if \"annÃ©e prochaine\" in question or \"prochain\" in question:\n",
    "        last_year = max(classement_files.keys())  # ğŸ“Œ DerniÃ¨re annÃ©e connue\n",
    "        df = classement_files[last_year]\n",
    "        champion = df.iloc[0]  # ğŸ“Œ Champion de l'annÃ©e derniÃ¨re\n",
    "\n",
    "        # ğŸ“Œ Faire une estimation de points\n",
    "        avg_growth = (df.iloc[0][\"Points\"] - classement_files[str(int(last_year) - 1)].iloc[0][\"Points\"]) / 2\n",
    "        predicted_points = df.iloc[0][\"Points\"] + avg_growth\n",
    "\n",
    "        return f\"Je pense que {champion['Pilote']} sera encore en tÃªte l'annÃ©e prochaine avec environ {int(predicted_points)} points.\"\n",
    "\n",
    "    return \"DÃ©solÃ©, je n'ai pas cette information.\"\n",
    "\n",
    "# ğŸ“Œ **Lancer le chatbot interactif**\n",
    "while True:\n",
    "    user_input = input(\"Pose une question sur la F1 : \")\n",
    "    if user_input.lower() == \"exit\":\n",
    "        break\n",
    "\n",
    "    response = search_f1_data(user_input)\n",
    "    print(\"F1Bot:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ ChargÃ© : classement F1 2020 â†’ 23 lignes, 5 colonnes\n",
      "ğŸ“‚ ChargÃ© : classement F1 2021 â†’ 21 lignes, 5 colonnes\n",
      "ğŸ“‚ ChargÃ© : classement F1 2022 â†’ 22 lignes, 5 colonnes\n",
      "ğŸ“‚ ChargÃ© : classement F1 2023 â†’ 22 lignes, 5 colonnes\n",
      "ğŸ“‚ ChargÃ© : classement F1 2024 â†’ 24 lignes, 5 colonnes\n",
      "F1Bot: DÃ©solÃ©, je n'ai pas cette information.\n",
      "F1Bot: DÃ©solÃ©, je n'ai pas cette information.\n",
      "F1Bot: Le champion du monde en 2021 Ã©tait Max Verstappen avec 395.5 points.\n",
      "F1Bot: DÃ©solÃ©, je n'ai pas cette information.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "# Charger le modÃ¨le NLP de spaCy\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "\n",
    "# ğŸ“Œ Fonction pour extraire l'annÃ©e du nom du fichier\n",
    "def extract_year_from_filename(filename):\n",
    "    match = re.search(r\"(20\\d{2})\", filename)  # Recherche une annÃ©e (ex: 2023)\n",
    "    return match.group(1) if match else \"inconnue\"\n",
    "\n",
    "# ğŸ“Œ Dossier contenant les fichiers CSV\n",
    "data_folder = \"data\"  # Mets le bon chemin ici\n",
    "\n",
    "# ğŸ“Œ Chargement automatique des fichiers de classement uniquement\n",
    "classement_files = {}\n",
    "\n",
    "for root, _, files in os.walk(data_folder):  \n",
    "    for file in files:\n",
    "        if file.startswith(\"classement_f1_\") and file.endswith(\".csv\"):\n",
    "            file_path = os.path.join(root, file)\n",
    "            year = extract_year_from_filename(file)\n",
    "            classement_files[year] = pd.read_csv(file_path)\n",
    "\n",
    "# ğŸ“Œ VÃ©rifier si les fichiers ont bien Ã©tÃ© chargÃ©s\n",
    "for year, df in classement_files.items():\n",
    "    print(f\"ğŸ“‚ ChargÃ© : classement F1 {year} â†’ {df.shape[0]} lignes, {df.shape[1]} colonnes\")\n",
    "\n",
    "# ğŸ“Œ **Moteur de recherche avancÃ© avec NLP**\n",
    "def search_f1_data(question):\n",
    "    doc = nlp(question.lower())  # Analyse NLP de la question\n",
    "    tokens = [token.text for token in doc]\n",
    "\n",
    "    # ğŸ“Œ Recherche du champion d'une annÃ©e spÃ©cifique\n",
    "    for year in classement_files.keys():\n",
    "        if str(year) in tokens and any(word in question for word in [\"champion\", \"gagnÃ©\", \"vainqueur\"]):\n",
    "            df = classement_files[year]\n",
    "            champion = df.iloc[0]  # ğŸ“Œ Le premier pilote est le champion\n",
    "            return f\"Le champion du monde en {year} Ã©tait {champion['Pilote']} avec {champion['Points']} points.\"\n",
    "\n",
    "    # ğŸ“Š ğŸ“Œ Qui a eu le plus de points en 5 ans ?\n",
    "    if \"plus\" in tokens and \"points\" in tokens and \"ans\" in tokens:\n",
    "        total_points = {}\n",
    "        for year, df in classement_files.items():\n",
    "            for _, row in df.iterrows():\n",
    "                pilote = row[\"Pilote\"]\n",
    "                points = row[\"Points\"]\n",
    "                if pilote in total_points:\n",
    "                    total_points[pilote] += points\n",
    "                else:\n",
    "                    total_points[pilote] = points\n",
    "\n",
    "        top_pilote = max(total_points, key=total_points.get)\n",
    "        max_points = total_points[top_pilote]\n",
    "\n",
    "        return f\"Le pilote qui a eu le plus de points ces 5 derniÃ¨res annÃ©es est {top_pilote} avec {max_points} points.\"\n",
    "\n",
    "    # ğŸ“‰ ğŸ“Œ Qui a eu le moins de points en 5 ans ?\n",
    "    if \"moins\" in tokens and \"points\" in tokens and \"ans\" in tokens:\n",
    "        total_points = {}\n",
    "        for year, df in classement_files.items():\n",
    "            for _, row in df.iterrows():\n",
    "                pilote = row[\"Pilote\"]\n",
    "                points = row[\"Points\"]\n",
    "                if pilote in total_points:\n",
    "                    total_points[pilote] += points\n",
    "                else:\n",
    "                    total_points[pilote] = points\n",
    "\n",
    "        worst_pilote = min(total_points, key=total_points.get)\n",
    "        min_points = total_points[worst_pilote]\n",
    "\n",
    "        return f\"Le pilote qui a eu le moins de points ces 5 derniÃ¨res annÃ©es est {worst_pilote} avec {min_points} points.\"\n",
    "\n",
    "    # ğŸ”® ğŸ“Œ PrÃ©diction du champion de l'annÃ©e prochaine\n",
    "    if \"annÃ©e prochaine\" in question or \"prochain\" in question or \"saison prochaine\" in question:\n",
    "        last_year = max(classement_files.keys())  # ğŸ“Œ DerniÃ¨re annÃ©e connue\n",
    "        df = classement_files[last_year]\n",
    "        champion = df.iloc[0]  # ğŸ“Œ Champion de l'annÃ©e derniÃ¨re\n",
    "\n",
    "        # ğŸ“Œ Faire une estimation des points\n",
    "        if str(int(last_year) - 1) in classement_files:\n",
    "            previous_df = classement_files[str(int(last_year) - 1)]\n",
    "            prev_champion = previous_df.iloc[0]\n",
    "            avg_growth = (df.iloc[0][\"Points\"] - prev_champion[\"Points\"]) / 2\n",
    "            predicted_points = df.iloc[0][\"Points\"] + avg_growth\n",
    "        else:\n",
    "            predicted_points = df.iloc[0][\"Points\"]  # Pas assez de donnÃ©es pour une tendance\n",
    "\n",
    "        return f\"Je pense que {champion['Pilote']} sera encore en tÃªte l'annÃ©e prochaine avec environ {int(predicted_points)} points.\"\n",
    "\n",
    "    return \"DÃ©solÃ©, je n'ai pas cette information.\"\n",
    "\n",
    "# ğŸ“Œ **Lancer le chatbot interactif**\n",
    "while True:\n",
    "    user_input = input(\"Pose une question sur la F1 : \")\n",
    "    if user_input.lower() == \"exit\":\n",
    "        break\n",
    "\n",
    "    response = search_f1_data(user_input)\n",
    "    print(\"F1Bot:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 23 key-value pairs and 291 tensors from mistral-7b-instruct-v0.1.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = hub\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  21:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  22:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = Q4_K - Medium\n",
      "print_info: file size   = 4.07 GiB (4.83 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 1\n",
      "load: control token:      2 '</s>' is not marked as EOG\n",
      "load: control token:      1 '<s>' is not marked as EOG\n",
      "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "load: special tokens cache size = 3\n",
      "load: token to piece cache size = 0.1637 MB\n",
      "print_info: arch             = llama\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 32768\n",
      "print_info: n_embd           = 4096\n",
      "print_info: n_layer          = 32\n",
      "print_info: n_head           = 32\n",
      "print_info: n_head_kv        = 8\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 4\n",
      "print_info: n_embd_k_gqa     = 1024\n",
      "print_info: n_embd_v_gqa     = 1024\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-05\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: n_ff             = 14336\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 0\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 10000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 32768\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: ssm_d_conv       = 0\n",
      "print_info: ssm_d_inner      = 0\n",
      "print_info: ssm_d_state      = 0\n",
      "print_info: ssm_dt_rank      = 0\n",
      "print_info: ssm_dt_b_c_rms   = 0\n",
      "print_info: model type       = 7B\n",
      "print_info: model params     = 7.24 B\n",
      "print_info: general.name     = hub\n",
      "print_info: vocab type       = SPM\n",
      "print_info: n_vocab          = 32000\n",
      "print_info: n_merges         = 0\n",
      "print_info: BOS token        = 1 '<s>'\n",
      "print_info: EOS token        = 2 '</s>'\n",
      "print_info: UNK token        = 0 '<unk>'\n",
      "print_info: LF token         = 13 '<0x0A>'\n",
      "print_info: EOG token        = 2 '</s>'\n",
      "print_info: max token length = 48\n",
      "load_tensors: layer   0 assigned to device CPU\n",
      "load_tensors: layer   1 assigned to device CPU\n",
      "load_tensors: layer   2 assigned to device CPU\n",
      "load_tensors: layer   3 assigned to device CPU\n",
      "load_tensors: layer   4 assigned to device CPU\n",
      "load_tensors: layer   5 assigned to device CPU\n",
      "load_tensors: layer   6 assigned to device CPU\n",
      "load_tensors: layer   7 assigned to device CPU\n",
      "load_tensors: layer   8 assigned to device CPU\n",
      "load_tensors: layer   9 assigned to device CPU\n",
      "load_tensors: layer  10 assigned to device CPU\n",
      "load_tensors: layer  11 assigned to device CPU\n",
      "load_tensors: layer  12 assigned to device CPU\n",
      "load_tensors: layer  13 assigned to device CPU\n",
      "load_tensors: layer  14 assigned to device CPU\n",
      "load_tensors: layer  15 assigned to device CPU\n",
      "load_tensors: layer  16 assigned to device CPU\n",
      "load_tensors: layer  17 assigned to device CPU\n",
      "load_tensors: layer  18 assigned to device CPU\n",
      "load_tensors: layer  19 assigned to device CPU\n",
      "load_tensors: layer  20 assigned to device CPU\n",
      "load_tensors: layer  21 assigned to device CPU\n",
      "load_tensors: layer  22 assigned to device CPU\n",
      "load_tensors: layer  23 assigned to device CPU\n",
      "load_tensors: layer  24 assigned to device CPU\n",
      "load_tensors: layer  25 assigned to device CPU\n",
      "load_tensors: layer  26 assigned to device CPU\n",
      "load_tensors: layer  27 assigned to device CPU\n",
      "load_tensors: layer  28 assigned to device CPU\n",
      "load_tensors: layer  29 assigned to device CPU\n",
      "load_tensors: layer  30 assigned to device CPU\n",
      "load_tensors: layer  31 assigned to device CPU\n",
      "load_tensors: layer  32 assigned to device CPU\n",
      "load_tensors: tensor 'token_embd.weight' (q4_K) (and 290 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "load_tensors:   CPU_Mapped model buffer size =  4165.37 MiB\n",
      "llama_init_from_model: n_seq_max     = 1\n",
      "llama_init_from_model: n_ctx         = 2048\n",
      "llama_init_from_model: n_ctx_per_seq = 2048\n",
      "llama_init_from_model: n_batch       = 512\n",
      "llama_init_from_model: n_ubatch      = 512\n",
      "llama_init_from_model: flash_attn    = 0\n",
      "llama_init_from_model: freq_base     = 10000.0\n",
      "llama_init_from_model: freq_scale    = 1\n",
      "llama_init_from_model: n_ctx_per_seq (2048) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
      "llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1\n",
      "llama_kv_cache_init: layer 0: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 1: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 2: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 3: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 4: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 5: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 6: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 7: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 8: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 9: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 10: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 11: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 12: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 13: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 14: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 15: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 16: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 17: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 18: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 19: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 20: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 21: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 22: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 23: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 24: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 25: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 26: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 27: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 28: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 29: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 30: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 31: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init:        CPU KV buffer size =   256.00 MiB\n",
      "llama_init_from_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_init_from_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_init_from_model:        CPU compute buffer size =   164.01 MiB\n",
      "llama_init_from_model: graph nodes  = 1030\n",
      "llama_init_from_model: graph splits = 1\n",
      "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'general.name': 'hub', 'general.architecture': 'llama', 'llama.context_length': '32768', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'llama.feed_forward_length': '14336', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '2', 'general.file_type': '15', 'llama.attention.head_count_kv': '8', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '10000.000000', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token + ' ' }}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\"}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Guessed chat format: mistral-instruct\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ ChargÃ© : classement F1 2020 â†’ 23 lignes, 5 colonnes\n",
      "ğŸ“‚ ChargÃ© : classement F1 2021 â†’ 21 lignes, 5 colonnes\n",
      "ğŸ“‚ ChargÃ© : classement F1 2022 â†’ 22 lignes, 5 colonnes\n",
      "ğŸ“‚ ChargÃ© : classement F1 2023 â†’ 22 lignes, 5 colonnes\n",
      "ğŸ“‚ ChargÃ© : classement F1 2024 â†’ 24 lignes, 5 colonnes\n",
      "F1Bot: DÃ©solÃ©, je n'ai pas cette information.\n",
      "F1Bot: DÃ©solÃ©, je n'ai pas cette information.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 96\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;66;03m# ğŸ“Œ Si on a une rÃ©ponse, demander au LLM de reformuler\u001b[39;00m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m search_result \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDÃ©solÃ©, je n\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mai pas cette information.\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 96\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msearch_result\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     98\u001b[0m     response \u001b[38;5;241m=\u001b[39m search_result\n",
      "Cell \u001b[1;32mIn[29], line 82\u001b[0m, in \u001b[0;36mgenerate_response\u001b[1;34m(question, search_result)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgenerate_response\u001b[39m(question, search_result):\n\u001b[0;32m     73\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;124m    Tu es un expert en Formule 1. Un utilisateur te pose la question :\u001b[39m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquestion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;124m    RÃ©ponds de maniÃ¨re naturelle et claire.\u001b[39m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m---> 82\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mllm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\cantfly\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\llama_cpp\\llama.py:1902\u001b[0m, in \u001b[0;36mLlama.__call__\u001b[1;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[0m\n\u001b[0;32m   1838\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\n\u001b[0;32m   1839\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1840\u001b[0m     prompt: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1864\u001b[0m     logit_bias: Optional[Dict[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1865\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[CreateCompletionResponse, Iterator[CreateCompletionStreamResponse]]:\n\u001b[0;32m   1866\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Generate text from a prompt.\u001b[39;00m\n\u001b[0;32m   1867\u001b[0m \n\u001b[0;32m   1868\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1900\u001b[0m \u001b[38;5;124;03m        Response object containing the generated text.\u001b[39;00m\n\u001b[0;32m   1901\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1902\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_completion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1903\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1904\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuffix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuffix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1905\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1906\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1907\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1908\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmin_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1909\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtypical_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtypical_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1910\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1911\u001b[0m \u001b[43m        \u001b[49m\u001b[43mecho\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mecho\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1912\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1913\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1914\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1915\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepeat_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepeat_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1916\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1917\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1918\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtfs_z\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtfs_z\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmirostat_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmirostat_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmirostat_tau\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmirostat_tau\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1922\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmirostat_eta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmirostat_eta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1925\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1926\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrammar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrammar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1927\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1928\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\cantfly\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\llama_cpp\\llama.py:1835\u001b[0m, in \u001b[0;36mLlama.create_completion\u001b[1;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[0m\n\u001b[0;32m   1833\u001b[0m     chunks: Iterator[CreateCompletionStreamResponse] \u001b[38;5;241m=\u001b[39m completion_or_chunks\n\u001b[0;32m   1834\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m chunks\n\u001b[1;32m-> 1835\u001b[0m completion: Completion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcompletion_or_chunks\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m   1836\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m completion\n",
      "File \u001b[1;32mc:\\Users\\cantfly\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\llama_cpp\\llama.py:1320\u001b[0m, in \u001b[0;36mLlama._create_completion\u001b[1;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[0m\n\u001b[0;32m   1318\u001b[0m finish_reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlength\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1319\u001b[0m multibyte_fix \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1320\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[0;32m   1321\u001b[0m     prompt_tokens,\n\u001b[0;32m   1322\u001b[0m     top_k\u001b[38;5;241m=\u001b[39mtop_k,\n\u001b[0;32m   1323\u001b[0m     top_p\u001b[38;5;241m=\u001b[39mtop_p,\n\u001b[0;32m   1324\u001b[0m     min_p\u001b[38;5;241m=\u001b[39mmin_p,\n\u001b[0;32m   1325\u001b[0m     typical_p\u001b[38;5;241m=\u001b[39mtypical_p,\n\u001b[0;32m   1326\u001b[0m     temp\u001b[38;5;241m=\u001b[39mtemperature,\n\u001b[0;32m   1327\u001b[0m     tfs_z\u001b[38;5;241m=\u001b[39mtfs_z,\n\u001b[0;32m   1328\u001b[0m     mirostat_mode\u001b[38;5;241m=\u001b[39mmirostat_mode,\n\u001b[0;32m   1329\u001b[0m     mirostat_tau\u001b[38;5;241m=\u001b[39mmirostat_tau,\n\u001b[0;32m   1330\u001b[0m     mirostat_eta\u001b[38;5;241m=\u001b[39mmirostat_eta,\n\u001b[0;32m   1331\u001b[0m     frequency_penalty\u001b[38;5;241m=\u001b[39mfrequency_penalty,\n\u001b[0;32m   1332\u001b[0m     presence_penalty\u001b[38;5;241m=\u001b[39mpresence_penalty,\n\u001b[0;32m   1333\u001b[0m     repeat_penalty\u001b[38;5;241m=\u001b[39mrepeat_penalty,\n\u001b[0;32m   1334\u001b[0m     stopping_criteria\u001b[38;5;241m=\u001b[39mstopping_criteria,\n\u001b[0;32m   1335\u001b[0m     logits_processor\u001b[38;5;241m=\u001b[39mlogits_processor,\n\u001b[0;32m   1336\u001b[0m     grammar\u001b[38;5;241m=\u001b[39mgrammar,\n\u001b[0;32m   1337\u001b[0m ):\n\u001b[0;32m   1338\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m llama_cpp\u001b[38;5;241m.\u001b[39mllama_token_is_eog(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model\u001b[38;5;241m.\u001b[39mvocab, token):\n\u001b[0;32m   1339\u001b[0m         text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdetokenize(completion_tokens, prev_tokens\u001b[38;5;241m=\u001b[39mprompt_tokens)\n",
      "File \u001b[1;32mc:\\Users\\cantfly\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\llama_cpp\\llama.py:912\u001b[0m, in \u001b[0;36mLlama.generate\u001b[1;34m(self, tokens, top_k, top_p, min_p, typical_p, temp, repeat_penalty, reset, frequency_penalty, presence_penalty, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, penalize_nl, logits_processor, stopping_criteria, grammar)\u001b[0m\n\u001b[0;32m    910\u001b[0m \u001b[38;5;66;03m# Eval and sample\u001b[39;00m\n\u001b[0;32m    911\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 912\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    913\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m sample_idx \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_tokens:\n\u001b[0;32m    914\u001b[0m         token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample(\n\u001b[0;32m    915\u001b[0m             top_k\u001b[38;5;241m=\u001b[39mtop_k,\n\u001b[0;32m    916\u001b[0m             top_p\u001b[38;5;241m=\u001b[39mtop_p,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    930\u001b[0m             idx\u001b[38;5;241m=\u001b[39msample_idx,\n\u001b[0;32m    931\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\cantfly\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\llama_cpp\\llama.py:646\u001b[0m, in \u001b[0;36mLlama.eval\u001b[1;34m(self, tokens)\u001b[0m\n\u001b[0;32m    642\u001b[0m n_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch)\n\u001b[0;32m    643\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch\u001b[38;5;241m.\u001b[39mset_batch(\n\u001b[0;32m    644\u001b[0m     batch\u001b[38;5;241m=\u001b[39mbatch, n_past\u001b[38;5;241m=\u001b[39mn_past, logits_all\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext_params\u001b[38;5;241m.\u001b[39mlogits_all\n\u001b[0;32m    645\u001b[0m )\n\u001b[1;32m--> 646\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    647\u001b[0m \u001b[38;5;66;03m# Save tokens\u001b[39;00m\n\u001b[0;32m    648\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_ids[n_past : n_past \u001b[38;5;241m+\u001b[39m n_tokens] \u001b[38;5;241m=\u001b[39m batch\n",
      "File \u001b[1;32mc:\\Users\\cantfly\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\llama_cpp\\_internals.py:306\u001b[0m, in \u001b[0;36mLlamaContext.decode\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch: LlamaBatch):\n\u001b[1;32m--> 306\u001b[0m     return_code \u001b[38;5;241m=\u001b[39m \u001b[43mllama_cpp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllama_decode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    307\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    308\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    309\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    310\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m return_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    311\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama_decode returned \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreturn_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from llama_cpp import Llama  # ğŸš€ ModÃ¨le LLM local\n",
    "\n",
    "# ğŸ“Œ Charger le modÃ¨le LLM (Mistral 7B GGUF)\n",
    "llm = Llama(model_path=\"mistral-7b-instruct-v0.1.Q4_K_M.gguf\", n_ctx=2048)\n",
    "\n",
    "# ğŸ“Œ Fonction pour extraire l'annÃ©e du fichier CSV\n",
    "def extract_year_from_filename(filename):\n",
    "    match = re.search(r\"(20\\d{2})\", filename)  # Recherche une annÃ©e (ex: 2023)\n",
    "    return match.group(1) if match else \"inconnue\"\n",
    "\n",
    "# ğŸ“Œ Dossier contenant les fichiers CSV\n",
    "data_folder = \"data\"\n",
    "\n",
    "# ğŸ“Œ Chargement des fichiers de classement uniquement\n",
    "classement_files = {}\n",
    "\n",
    "for root, _, files in os.walk(data_folder):  \n",
    "    for file in files:\n",
    "        if file.startswith(\"classement_f1_\") and file.endswith(\".csv\"):\n",
    "            file_path = os.path.join(root, file)\n",
    "            year = extract_year_from_filename(file)\n",
    "            classement_files[year] = pd.read_csv(file_path)\n",
    "\n",
    "# ğŸ“Œ VÃ©rifier que les fichiers sont bien chargÃ©s\n",
    "for year, df in classement_files.items():\n",
    "    print(f\"ğŸ“‚ ChargÃ© : classement F1 {year} â†’ {df.shape[0]} lignes, {df.shape[1]} colonnes\")\n",
    "\n",
    "# ğŸ“Œ **Moteur de recherche des rÃ©sultats F1**\n",
    "def search_f1_data(question):\n",
    "    question = question.lower()\n",
    "\n",
    "    # ğŸ“Œ Recherche du champion d'une annÃ©e spÃ©cifique\n",
    "    match = re.search(r\"(20\\d{2})\", question)\n",
    "    if match and \"champion\" in question:\n",
    "        year = match.group(1)\n",
    "        if year in classement_files:\n",
    "            df = classement_files[year]\n",
    "            champion = df.iloc[0]  # ğŸ“Œ Le premier pilote est le champion\n",
    "            return f\"Le champion du monde en {year} Ã©tait {champion['Pilote']} avec {champion['Points']} points.\"\n",
    "\n",
    "    # ğŸ“Š ğŸ“Œ Qui a eu le plus de points en 5 ans ?\n",
    "    if \"plus de points\" in question:\n",
    "        total_points = {}\n",
    "        for year, df in classement_files.items():\n",
    "            for _, row in df.iterrows():\n",
    "                pilote = row[\"Pilote\"]\n",
    "                points = row[\"Points\"]\n",
    "                if pilote in total_points:\n",
    "                    total_points[pilote] += points\n",
    "                else:\n",
    "                    total_points[pilote] = points\n",
    "\n",
    "        top_pilote = max(total_points, key=total_points.get)\n",
    "        max_points = total_points[top_pilote]\n",
    "\n",
    "        return f\"Le pilote qui a eu le plus de points ces 5 derniÃ¨res annÃ©es est {top_pilote} avec {max_points} points.\"\n",
    "\n",
    "    # ğŸ”® ğŸ“Œ PrÃ©diction du champion de l'annÃ©e prochaine\n",
    "    if \"annÃ©e prochaine\" in question or \"qui sera champion\" in question:\n",
    "        last_year = max(classement_files.keys())  # ğŸ“Œ DerniÃ¨re annÃ©e connue\n",
    "        df = classement_files[last_year]\n",
    "        champion = df.iloc[0]  # ğŸ“Œ Champion de l'annÃ©e derniÃ¨re\n",
    "\n",
    "        return f\"Je pense que {champion['Pilote']} a de grandes chances dâ€™Ãªtre en tÃªte lâ€™annÃ©e prochaine.\"\n",
    "\n",
    "    return \"DÃ©solÃ©, je n'ai pas cette information.\"\n",
    "\n",
    "# ğŸ“Œ **GÃ©nÃ©rer une rÃ©ponse avec le LLM**\n",
    "def generate_response(question, search_result):\n",
    "    prompt = f\"\"\"\n",
    "    Tu es un expert en Formule 1. Un utilisateur te pose la question :\n",
    "    \"{question}\"\n",
    "\n",
    "    Voici l'information brute trouvÃ©e dans les fichiers CSV :\n",
    "    \"{search_result}\"\n",
    "\n",
    "    RÃ©ponds de maniÃ¨re naturelle et claire.\n",
    "    \"\"\"\n",
    "    output = llm(prompt, max_tokens=256)\n",
    "    return output[\"choices\"][0][\"text\"]\n",
    "\n",
    "# ğŸ“Œ **Lancer le chatbot interactif**\n",
    "while True:\n",
    "    user_input = input(\"Pose une question sur la F1 : \")\n",
    "    if user_input.lower() == \"exit\":\n",
    "        break\n",
    "\n",
    "    # ğŸ“Œ Recherche dans les fichiers CSV\n",
    "    search_result = search_f1_data(user_input)\n",
    "\n",
    "    # ğŸ“Œ Si on a une rÃ©ponse, demander au LLM de reformuler\n",
    "    if search_result != \"DÃ©solÃ©, je n'ai pas cette information.\":\n",
    "        response = generate_response(user_input, search_result)\n",
    "    else:\n",
    "        response = search_result\n",
    "\n",
    "    print(\"F1Bot:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 23 key-value pairs and 291 tensors from mistral-7b-instruct-v0.1.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = hub\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  21:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  22:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = Q4_K - Medium\n",
      "print_info: file size   = 4.07 GiB (4.83 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 1\n",
      "load: control token:      2 '</s>' is not marked as EOG\n",
      "load: control token:      1 '<s>' is not marked as EOG\n",
      "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "load: special tokens cache size = 3\n",
      "load: token to piece cache size = 0.1637 MB\n",
      "print_info: arch             = llama\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 32768\n",
      "print_info: n_embd           = 4096\n",
      "print_info: n_layer          = 32\n",
      "print_info: n_head           = 32\n",
      "print_info: n_head_kv        = 8\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 4\n",
      "print_info: n_embd_k_gqa     = 1024\n",
      "print_info: n_embd_v_gqa     = 1024\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-05\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: n_ff             = 14336\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 0\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 10000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 32768\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: ssm_d_conv       = 0\n",
      "print_info: ssm_d_inner      = 0\n",
      "print_info: ssm_d_state      = 0\n",
      "print_info: ssm_dt_rank      = 0\n",
      "print_info: ssm_dt_b_c_rms   = 0\n",
      "print_info: model type       = 7B\n",
      "print_info: model params     = 7.24 B\n",
      "print_info: general.name     = hub\n",
      "print_info: vocab type       = SPM\n",
      "print_info: n_vocab          = 32000\n",
      "print_info: n_merges         = 0\n",
      "print_info: BOS token        = 1 '<s>'\n",
      "print_info: EOS token        = 2 '</s>'\n",
      "print_info: UNK token        = 0 '<unk>'\n",
      "print_info: LF token         = 13 '<0x0A>'\n",
      "print_info: EOG token        = 2 '</s>'\n",
      "print_info: max token length = 48\n",
      "load_tensors: layer   0 assigned to device CPU\n",
      "load_tensors: layer   1 assigned to device CPU\n",
      "load_tensors: layer   2 assigned to device CPU\n",
      "load_tensors: layer   3 assigned to device CPU\n",
      "load_tensors: layer   4 assigned to device CPU\n",
      "load_tensors: layer   5 assigned to device CPU\n",
      "load_tensors: layer   6 assigned to device CPU\n",
      "load_tensors: layer   7 assigned to device CPU\n",
      "load_tensors: layer   8 assigned to device CPU\n",
      "load_tensors: layer   9 assigned to device CPU\n",
      "load_tensors: layer  10 assigned to device CPU\n",
      "load_tensors: layer  11 assigned to device CPU\n",
      "load_tensors: layer  12 assigned to device CPU\n",
      "load_tensors: layer  13 assigned to device CPU\n",
      "load_tensors: layer  14 assigned to device CPU\n",
      "load_tensors: layer  15 assigned to device CPU\n",
      "load_tensors: layer  16 assigned to device CPU\n",
      "load_tensors: layer  17 assigned to device CPU\n",
      "load_tensors: layer  18 assigned to device CPU\n",
      "load_tensors: layer  19 assigned to device CPU\n",
      "load_tensors: layer  20 assigned to device CPU\n",
      "load_tensors: layer  21 assigned to device CPU\n",
      "load_tensors: layer  22 assigned to device CPU\n",
      "load_tensors: layer  23 assigned to device CPU\n",
      "load_tensors: layer  24 assigned to device CPU\n",
      "load_tensors: layer  25 assigned to device CPU\n",
      "load_tensors: layer  26 assigned to device CPU\n",
      "load_tensors: layer  27 assigned to device CPU\n",
      "load_tensors: layer  28 assigned to device CPU\n",
      "load_tensors: layer  29 assigned to device CPU\n",
      "load_tensors: layer  30 assigned to device CPU\n",
      "load_tensors: layer  31 assigned to device CPU\n",
      "load_tensors: layer  32 assigned to device CPU\n",
      "load_tensors: tensor 'token_embd.weight' (q4_K) (and 290 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "load_tensors:   CPU_Mapped model buffer size =  4165.37 MiB\n",
      "llama_init_from_model: n_seq_max     = 1\n",
      "llama_init_from_model: n_ctx         = 2048\n",
      "llama_init_from_model: n_ctx_per_seq = 2048\n",
      "llama_init_from_model: n_batch       = 512\n",
      "llama_init_from_model: n_ubatch      = 512\n",
      "llama_init_from_model: flash_attn    = 0\n",
      "llama_init_from_model: freq_base     = 10000.0\n",
      "llama_init_from_model: freq_scale    = 1\n",
      "llama_init_from_model: n_ctx_per_seq (2048) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
      "llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1\n",
      "llama_kv_cache_init: layer 0: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 1: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 2: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 3: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 4: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 5: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 6: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 7: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 8: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 9: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 10: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 11: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 12: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 13: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 14: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 15: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 16: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 17: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 18: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 19: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 20: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 21: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 22: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 23: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 24: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 25: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 26: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 27: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 28: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 29: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 30: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 31: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init:        CPU KV buffer size =   256.00 MiB\n",
      "llama_init_from_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_init_from_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_init_from_model:        CPU compute buffer size =   164.01 MiB\n",
      "llama_init_from_model: graph nodes  = 1030\n",
      "llama_init_from_model: graph splits = 1\n",
      "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'general.name': 'hub', 'general.architecture': 'llama', 'llama.context_length': '32768', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'llama.feed_forward_length': '14336', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '2', 'general.file_type': '15', 'llama.attention.head_count_kv': '8', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '10000.000000', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token + ' ' }}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\"}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Guessed chat format: mistral-instruct\n",
      "llama_perf_context_print:        load time =    2119.23 ms\n",
      "llama_perf_context_print: prompt eval time =    2118.98 ms /    18 tokens (  117.72 ms per token,     8.49 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5221.15 ms /    36 runs   (  145.03 ms per token,     6.90 tokens per second)\n",
      "llama_perf_context_print:       total time =    7367.12 ms /    54 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Le champion du monde de F1 en 2023 est le pilote Max Verstappen, qui court pour l'Ã©curie Red Bull Racing.\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "# Charger le modÃ¨le\n",
    "llm = Llama(model_path=\"mistral-7b-instruct-v0.1.Q4_K_M.gguf\", n_ctx=2048)\n",
    "\n",
    "# Test d'un prompt simple\n",
    "output = llm(\"Quel est le champion du monde de F1 en 2023 ?\", max_tokens=100)\n",
    "\n",
    "# Afficher la rÃ©ponse du modÃ¨le\n",
    "print(output[\"choices\"][0][\"text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 23 key-value pairs and 291 tensors from mistral-7b-instruct-v0.1.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = hub\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  21:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  22:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = Q4_K - Medium\n",
      "print_info: file size   = 4.07 GiB (4.83 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 1\n",
      "load: control token:      2 '</s>' is not marked as EOG\n",
      "load: control token:      1 '<s>' is not marked as EOG\n",
      "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "load: special tokens cache size = 3\n",
      "load: token to piece cache size = 0.1637 MB\n",
      "print_info: arch             = llama\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 32768\n",
      "print_info: n_embd           = 4096\n",
      "print_info: n_layer          = 32\n",
      "print_info: n_head           = 32\n",
      "print_info: n_head_kv        = 8\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 4\n",
      "print_info: n_embd_k_gqa     = 1024\n",
      "print_info: n_embd_v_gqa     = 1024\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-05\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: n_ff             = 14336\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 0\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 10000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 32768\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: ssm_d_conv       = 0\n",
      "print_info: ssm_d_inner      = 0\n",
      "print_info: ssm_d_state      = 0\n",
      "print_info: ssm_dt_rank      = 0\n",
      "print_info: ssm_dt_b_c_rms   = 0\n",
      "print_info: model type       = 7B\n",
      "print_info: model params     = 7.24 B\n",
      "print_info: general.name     = hub\n",
      "print_info: vocab type       = SPM\n",
      "print_info: n_vocab          = 32000\n",
      "print_info: n_merges         = 0\n",
      "print_info: BOS token        = 1 '<s>'\n",
      "print_info: EOS token        = 2 '</s>'\n",
      "print_info: UNK token        = 0 '<unk>'\n",
      "print_info: LF token         = 13 '<0x0A>'\n",
      "print_info: EOG token        = 2 '</s>'\n",
      "print_info: max token length = 48\n",
      "load_tensors: layer   0 assigned to device CPU\n",
      "load_tensors: layer   1 assigned to device CPU\n",
      "load_tensors: layer   2 assigned to device CPU\n",
      "load_tensors: layer   3 assigned to device CPU\n",
      "load_tensors: layer   4 assigned to device CPU\n",
      "load_tensors: layer   5 assigned to device CPU\n",
      "load_tensors: layer   6 assigned to device CPU\n",
      "load_tensors: layer   7 assigned to device CPU\n",
      "load_tensors: layer   8 assigned to device CPU\n",
      "load_tensors: layer   9 assigned to device CPU\n",
      "load_tensors: layer  10 assigned to device CPU\n",
      "load_tensors: layer  11 assigned to device CPU\n",
      "load_tensors: layer  12 assigned to device CPU\n",
      "load_tensors: layer  13 assigned to device CPU\n",
      "load_tensors: layer  14 assigned to device CPU\n",
      "load_tensors: layer  15 assigned to device CPU\n",
      "load_tensors: layer  16 assigned to device CPU\n",
      "load_tensors: layer  17 assigned to device CPU\n",
      "load_tensors: layer  18 assigned to device CPU\n",
      "load_tensors: layer  19 assigned to device CPU\n",
      "load_tensors: layer  20 assigned to device CPU\n",
      "load_tensors: layer  21 assigned to device CPU\n",
      "load_tensors: layer  22 assigned to device CPU\n",
      "load_tensors: layer  23 assigned to device CPU\n",
      "load_tensors: layer  24 assigned to device CPU\n",
      "load_tensors: layer  25 assigned to device CPU\n",
      "load_tensors: layer  26 assigned to device CPU\n",
      "load_tensors: layer  27 assigned to device CPU\n",
      "load_tensors: layer  28 assigned to device CPU\n",
      "load_tensors: layer  29 assigned to device CPU\n",
      "load_tensors: layer  30 assigned to device CPU\n",
      "load_tensors: layer  31 assigned to device CPU\n",
      "load_tensors: layer  32 assigned to device CPU\n",
      "load_tensors: tensor 'token_embd.weight' (q4_K) (and 290 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "load_tensors:   CPU_Mapped model buffer size =  4165.37 MiB\n",
      "llama_init_from_model: n_seq_max     = 1\n",
      "llama_init_from_model: n_ctx         = 2048\n",
      "llama_init_from_model: n_ctx_per_seq = 2048\n",
      "llama_init_from_model: n_batch       = 512\n",
      "llama_init_from_model: n_ubatch      = 512\n",
      "llama_init_from_model: flash_attn    = 0\n",
      "llama_init_from_model: freq_base     = 10000.0\n",
      "llama_init_from_model: freq_scale    = 1\n",
      "llama_init_from_model: n_ctx_per_seq (2048) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
      "llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1\n",
      "llama_kv_cache_init: layer 0: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 1: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 2: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 3: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 4: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 5: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 6: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 7: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 8: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 9: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 10: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 11: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 12: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 13: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 14: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 15: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 16: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 17: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 18: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 19: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 20: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 21: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 22: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 23: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 24: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 25: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 26: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 27: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 28: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 29: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 30: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 31: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init:        CPU KV buffer size =   256.00 MiB\n",
      "llama_init_from_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_init_from_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_init_from_model:        CPU compute buffer size =   164.01 MiB\n",
      "llama_init_from_model: graph nodes  = 1030\n",
      "llama_init_from_model: graph splits = 1\n",
      "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'general.name': 'hub', 'general.architecture': 'llama', 'llama.context_length': '32768', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'llama.feed_forward_length': '14336', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '2', 'general.file_type': '15', 'llama.attention.head_count_kv': '8', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '10000.000000', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token + ' ' }}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\"}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Guessed chat format: mistral-instruct\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ ChargÃ© : classement F1 2020 â†’ 23 lignes, 5 colonnes\n",
      "ğŸ“‚ ChargÃ© : classement F1 2021 â†’ 21 lignes, 5 colonnes\n",
      "ğŸ“‚ ChargÃ© : classement F1 2022 â†’ 22 lignes, 5 colonnes\n",
      "ğŸ“‚ ChargÃ© : classement F1 2023 â†’ 22 lignes, 5 colonnes\n",
      "ğŸ“‚ ChargÃ© : classement F1 2024 â†’ 24 lignes, 5 colonnes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   42682.07 ms\n",
      "llama_perf_context_print: prompt eval time =   42681.01 ms /   530 tokens (   80.53 ms per token,    12.42 tokens per second)\n",
      "llama_perf_context_print:        eval time =   39297.90 ms /   255 runs   (  154.11 ms per token,     6.49 tokens per second)\n",
      "llama_perf_context_print:       total time =   82258.61 ms /   785 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1Bot: \n",
      "    Voici le classement en 2023 de Formule 1 :\n",
      "\n",
      "    Max Verstappen (Red Bull) - 575 points et 19 victoires\n",
      "    Sergio PÃ©rez (Red Bull) - 285 points et 2 victoires\n",
      "    Lewis Hamilton (Mercedes) - 234 points et 0 victoires\n",
      "    Fernando Alonso (Aston Martin) - 206 points et 0 victoires\n",
      "    Charles Leclerc (Ferrari) - 206 points et 0 victoires\n",
      "    Lando Norris (McLaren) - 205 points et 0 victoires\n",
      "    Carlos Sainz (Ferrari) - 200 points et 1 victoire\n",
      "    George Russell (Mercedes) - 175 points et 0 victoires\n",
      "    Oscar Piastri (McLaren) - 97 points et 0 victoires\n",
      "    Lance Stroll (Aston Martin) - 74 points et 0 victoires\n",
      "    Pierre Gasly (Alpine F1 Team) - 62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 37 prefix-match hit, remaining 535 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   42682.07 ms\n",
      "llama_perf_context_print: prompt eval time =   41581.65 ms /   535 tokens (   77.72 ms per token,    12.87 tokens per second)\n",
      "llama_perf_context_print:        eval time =   22615.78 ms /   147 runs   (  153.85 ms per token,     6.50 tokens per second)\n",
      "llama_perf_context_print:       total time =   64331.83 ms /   682 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1Bot: \n",
      "    En 2024, Max Verstappen a Ã©tÃ© le pilote de Red Bull et a remportÃ© 9 courses, obtenu 437 points et terminÃ© 1er du championnat. Lando Norris et George Russell, Ã©galement du Red Bull, ont fini 2e et 6e du championnat respectivement. Charles Leclerc du Ferrari a terminÃ© 3e avec 356 points, tandis que Pierre Gasly du Red Bull et Nico HÃ¼lkenberg de l'Haas F1 Team ont fini 10e et 11e du championnat avec respectivement 42 et 41 points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 31 prefix-match hit, remaining 502 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   42682.07 ms\n",
      "llama_perf_context_print: prompt eval time =   38990.30 ms /   502 tokens (   77.67 ms per token,    12.87 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5127.92 ms /    34 runs   (  150.82 ms per token,     6.63 tokens per second)\n",
      "llama_perf_context_print:       total time =   44144.55 ms /   536 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1Bot: \n",
      "    Max Verstappen was the winner of the Formula 1 Championship in 2023 with 575 points and 19 victories.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from llama_cpp import Llama  # ğŸš€ ModÃ¨le LLM local\n",
    "\n",
    "# ğŸ“Œ Charger le modÃ¨le LLM (Mistral 7B GGUF)\n",
    "llm = Llama(model_path=\"mistral-7b-instruct-v0.1.Q4_K_M.gguf\", n_ctx=2048)\n",
    "\n",
    "# ğŸ“Œ Fonction pour extraire l'annÃ©e du fichier CSV\n",
    "def extract_year_from_filename(filename):\n",
    "    match = re.search(r\"(20\\d{2})\", filename)  # Recherche une annÃ©e (ex: 2023)\n",
    "    return match.group(1) if match else \"inconnue\"\n",
    "\n",
    "# ğŸ“Œ Dossier contenant les fichiers CSV\n",
    "data_folder = \"data\"\n",
    "\n",
    "# ğŸ“Œ Chargement des fichiers de classement uniquement\n",
    "classement_files = {}\n",
    "\n",
    "for root, _, files in os.walk(data_folder):  \n",
    "    for file in files:\n",
    "        if file.startswith(\"classement_f1_\") and file.endswith(\".csv\"):\n",
    "            file_path = os.path.join(root, file)\n",
    "            year = extract_year_from_filename(file)\n",
    "            classement_files[year] = pd.read_csv(file_path)\n",
    "\n",
    "# ğŸ“Œ VÃ©rifier que les fichiers sont bien chargÃ©s\n",
    "for year, df in classement_files.items():\n",
    "    print(f\"ğŸ“‚ ChargÃ© : classement F1 {year} â†’ {df.shape[0]} lignes, {df.shape[1]} colonnes\")\n",
    "\n",
    "def search_f1_data(question):\n",
    "    question = question.lower()\n",
    "    search_result = None\n",
    "\n",
    "    # ğŸ“Œ VÃ©rifier s'il y a une annÃ©e dans la question\n",
    "    match = re.search(r\"(20\\d{2})\", question)\n",
    "    year = match.group(1) if match else None\n",
    "\n",
    "    # ğŸ“Œ VÃ©rifier si la question concerne le classement\n",
    "    if \"classement\" in question and year:\n",
    "        file_key = f\"classement_f1_{year}.csv\"\n",
    "        if year in classement_files:\n",
    "            df = classement_files[year]\n",
    "            search_result = df.to_string(index=False)  # Convertir le DataFrame en texte brut\n",
    "\n",
    "    # ğŸ“Œ VÃ©rifier si la question concerne le calendrier des courses\n",
    "    elif \"calendrier\" in question and year:\n",
    "        file_key = f\"calendrier_f1_{year}.csv\"\n",
    "        calendar_file = os.path.join(data_folder, \"calendrier\", file_key)\n",
    "        if os.path.exists(calendar_file):\n",
    "            df = pd.read_csv(calendar_file)\n",
    "            search_result = df.to_string(index=False)\n",
    "\n",
    "    # ğŸ“Œ VÃ©rifier si la question concerne les circuits\n",
    "    elif \"circuit\" in question and year:\n",
    "        file_key = f\"circuits_f1_{year}.csv\"\n",
    "        circuit_file = os.path.join(data_folder, \"circuits\", file_key)\n",
    "        if os.path.exists(circuit_file):\n",
    "            df = pd.read_csv(circuit_file)\n",
    "            search_result = df.to_string(index=False)\n",
    "\n",
    "    # ğŸ“Œ VÃ©rifier si la question concerne les constructeurs\n",
    "    elif \"constructeur\" in question and year:\n",
    "        file_key = f\"constructeurs_f1_{year}.csv\"\n",
    "        constructor_file = os.path.join(data_folder, \"constructeurs\", file_key)\n",
    "        if os.path.exists(constructor_file):\n",
    "            df = pd.read_csv(constructor_file)\n",
    "            search_result = df.to_string(index=False)\n",
    "\n",
    "    if search_result:\n",
    "        return f\"Voici les donnÃ©es extraites du fichier **{file_key}** :\\n\\n{search_result}\"\n",
    "    \n",
    "    print(question)\n",
    "    print(search_result)\n",
    "\n",
    "    return \"DÃ©solÃ©, je n'ai pas trouvÃ© de donnÃ©es correspondantes.\"\n",
    "\n",
    "\n",
    "# ğŸ“Œ **GÃ©nÃ©rer une rÃ©ponse avec le LLM**\n",
    "def generate_response(question, search_result):\n",
    "    prompt = f\"\"\"\n",
    "    [INST] \n",
    "    Tu es un expert en Formule 1. Un utilisateur te pose la question :\n",
    "    \"{question}\"\n",
    "\n",
    "    Voici les donnÃ©es brutes extraites des fichiers CSV :\n",
    "    {search_result}\n",
    "\n",
    "    Analyse les donnÃ©es et donne une rÃ©ponse claire et concise.\n",
    "    [/INST]\n",
    "    \"\"\"\n",
    "\n",
    "    output = llm(prompt, max_tokens=256)\n",
    "    return output[\"choices\"][0][\"text\"]\n",
    "\n",
    "\n",
    "\n",
    "# ğŸ“Œ **Lancer le chatbot interactif**\n",
    "while True:\n",
    "    user_input = input(\"Pose une question sur la F1 : \")\n",
    "    if user_input.lower() == \"exit\":\n",
    "        break\n",
    "\n",
    "    # ğŸ” Recherche dans les fichiers CSV\n",
    "    search_result = search_f1_data(user_input)\n",
    "\n",
    "    # ğŸ§  GÃ©nÃ©ration de la rÃ©ponse avec le LLM\n",
    "    response = generate_response(user_input, search_result)\n",
    "    print(\"F1Bot:\", response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 23 key-value pairs and 291 tensors from mistral-7b-instruct-v0.1.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = hub\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  21:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  22:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = Q4_K - Medium\n",
      "print_info: file size   = 4.07 GiB (4.83 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 1\n",
      "load: control token:      2 '</s>' is not marked as EOG\n",
      "load: control token:      1 '<s>' is not marked as EOG\n",
      "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "load: special tokens cache size = 3\n",
      "load: token to piece cache size = 0.1637 MB\n",
      "print_info: arch             = llama\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 32768\n",
      "print_info: n_embd           = 4096\n",
      "print_info: n_layer          = 32\n",
      "print_info: n_head           = 32\n",
      "print_info: n_head_kv        = 8\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 4\n",
      "print_info: n_embd_k_gqa     = 1024\n",
      "print_info: n_embd_v_gqa     = 1024\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-05\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: n_ff             = 14336\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 0\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 10000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 32768\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: ssm_d_conv       = 0\n",
      "print_info: ssm_d_inner      = 0\n",
      "print_info: ssm_d_state      = 0\n",
      "print_info: ssm_dt_rank      = 0\n",
      "print_info: ssm_dt_b_c_rms   = 0\n",
      "print_info: model type       = 7B\n",
      "print_info: model params     = 7.24 B\n",
      "print_info: general.name     = hub\n",
      "print_info: vocab type       = SPM\n",
      "print_info: n_vocab          = 32000\n",
      "print_info: n_merges         = 0\n",
      "print_info: BOS token        = 1 '<s>'\n",
      "print_info: EOS token        = 2 '</s>'\n",
      "print_info: UNK token        = 0 '<unk>'\n",
      "print_info: LF token         = 13 '<0x0A>'\n",
      "print_info: EOG token        = 2 '</s>'\n",
      "print_info: max token length = 48\n",
      "load_tensors: layer   0 assigned to device CPU\n",
      "load_tensors: layer   1 assigned to device CPU\n",
      "load_tensors: layer   2 assigned to device CPU\n",
      "load_tensors: layer   3 assigned to device CPU\n",
      "load_tensors: layer   4 assigned to device CPU\n",
      "load_tensors: layer   5 assigned to device CPU\n",
      "load_tensors: layer   6 assigned to device CPU\n",
      "load_tensors: layer   7 assigned to device CPU\n",
      "load_tensors: layer   8 assigned to device CPU\n",
      "load_tensors: layer   9 assigned to device CPU\n",
      "load_tensors: layer  10 assigned to device CPU\n",
      "load_tensors: layer  11 assigned to device CPU\n",
      "load_tensors: layer  12 assigned to device CPU\n",
      "load_tensors: layer  13 assigned to device CPU\n",
      "load_tensors: layer  14 assigned to device CPU\n",
      "load_tensors: layer  15 assigned to device CPU\n",
      "load_tensors: layer  16 assigned to device CPU\n",
      "load_tensors: layer  17 assigned to device CPU\n",
      "load_tensors: layer  18 assigned to device CPU\n",
      "load_tensors: layer  19 assigned to device CPU\n",
      "load_tensors: layer  20 assigned to device CPU\n",
      "load_tensors: layer  21 assigned to device CPU\n",
      "load_tensors: layer  22 assigned to device CPU\n",
      "load_tensors: layer  23 assigned to device CPU\n",
      "load_tensors: layer  24 assigned to device CPU\n",
      "load_tensors: layer  25 assigned to device CPU\n",
      "load_tensors: layer  26 assigned to device CPU\n",
      "load_tensors: layer  27 assigned to device CPU\n",
      "load_tensors: layer  28 assigned to device CPU\n",
      "load_tensors: layer  29 assigned to device CPU\n",
      "load_tensors: layer  30 assigned to device CPU\n",
      "load_tensors: layer  31 assigned to device CPU\n",
      "load_tensors: layer  32 assigned to device CPU\n",
      "load_tensors: tensor 'token_embd.weight' (q4_K) (and 290 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "load_tensors:   CPU_Mapped model buffer size =  4165.37 MiB\n",
      "llama_init_from_model: n_seq_max     = 1\n",
      "llama_init_from_model: n_ctx         = 2048\n",
      "llama_init_from_model: n_ctx_per_seq = 2048\n",
      "llama_init_from_model: n_batch       = 512\n",
      "llama_init_from_model: n_ubatch      = 512\n",
      "llama_init_from_model: flash_attn    = 0\n",
      "llama_init_from_model: freq_base     = 10000.0\n",
      "llama_init_from_model: freq_scale    = 1\n",
      "llama_init_from_model: n_ctx_per_seq (2048) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
      "llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1\n",
      "llama_kv_cache_init: layer 0: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 1: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 2: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 3: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 4: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 5: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 6: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 7: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 8: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 9: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 10: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 11: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 12: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 13: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 14: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 15: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 16: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 17: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 18: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 19: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 20: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 21: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 22: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 23: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 24: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 25: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 26: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 27: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 28: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 29: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 30: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 31: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init:        CPU KV buffer size =   256.00 MiB\n",
      "llama_init_from_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_init_from_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_init_from_model:        CPU compute buffer size =   164.01 MiB\n",
      "llama_init_from_model: graph nodes  = 1030\n",
      "llama_init_from_model: graph splits = 1\n",
      "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'general.name': 'hub', 'general.architecture': 'llama', 'llama.context_length': '32768', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'llama.feed_forward_length': '14336', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '2', 'general.file_type': '15', 'llama.attention.head_count_kv': '8', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '10000.000000', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token + ' ' }}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\"}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Guessed chat format: mistral-instruct\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ ChargÃ© : classement F1 2020 â†’ 23 lignes, 5 colonnes\n",
      "ğŸ“‚ ChargÃ© : classement F1 2021 â†’ 21 lignes, 5 colonnes\n",
      "ğŸ“‚ ChargÃ© : classement F1 2022 â†’ 22 lignes, 5 colonnes\n",
      "ğŸ“‚ ChargÃ© : classement F1 2023 â†’ 22 lignes, 5 colonnes\n",
      "ğŸ“‚ ChargÃ© : classement F1 2024 â†’ 24 lignes, 5 colonnes\n",
      "2020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    9432.40 ms\n",
      "llama_perf_context_print: prompt eval time =    9431.36 ms /   114 tokens (   82.73 ms per token,    12.09 tokens per second)\n",
      "llama_perf_context_print:        eval time =    9513.16 ms /    65 runs   (  146.36 ms per token,     6.83 tokens per second)\n",
      "llama_perf_context_print:       total time =   18993.85 ms /   179 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1Bot: \n",
      "    Je regrette que je n'ai pas pu trouver les donnÃ©es correspondantes pour vous rÃ©pondre Ã  votre question.\n",
      "\n",
      "    Si vous pouvez fournir plus de dÃ©tails ou si vous avez d'autres questions, je serai heureux de vous aider.\n",
      "2020\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 110\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;66;03m# ğŸ” Recherche dans les fichiers CSV\u001b[39;00m\n\u001b[1;32m--> 110\u001b[0m data_results \u001b[38;5;241m=\u001b[39m \u001b[43msearch_f1_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;66;03m# ğŸ§  GÃ©nÃ©ration de la rÃ©ponse avec le LLM\u001b[39;00m\n\u001b[0;32m    113\u001b[0m response \u001b[38;5;241m=\u001b[39m generate_response(user_input, data_results)\n",
      "Cell \u001b[1;32mIn[43], line 54\u001b[0m, in \u001b[0;36msearch_f1_data\u001b[1;34m(question)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# ğŸ“Œ VÃ©rifier si la question concerne le classement\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclassement\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m question:\n\u001b[1;32m---> 54\u001b[0m     files_to_return \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclassement_f1_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m y \u001b[38;5;129;01min\u001b[39;00m year]\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28mprint\u001b[39m(files_to_return)\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# ğŸ“Œ VÃ©rifier si la question concerne le calendrier des courses\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from llama_cpp import Llama  # ğŸš€ ModÃ¨le LLM local\n",
    "\n",
    "# ğŸ“Œ Charger le modÃ¨le LLM (Mistral 7B GGUF)\n",
    "llm = Llama(model_path=\"mistral-7b-instruct-v0.1.Q4_K_M.gguf\", n_ctx=2048)\n",
    "\n",
    "# ğŸ“Œ Fonction pour extraire l'annÃ©e du fichier CSV\n",
    "def extract_year_from_filename(filename):\n",
    "    match = re.search(r\"(20\\d{2})\", filename)  # Recherche une annÃ©e (ex: 2023)\n",
    "    return match.group(1) if match else \"inconnue\"\n",
    "\n",
    "# ğŸ“Œ Dossier contenant les fichiers CSV\n",
    "data_folder = \"data\"\n",
    "\n",
    "# ğŸ“Œ Chargement des fichiers de classement uniquement\n",
    "classement_files = {}\n",
    "\n",
    "for root, _, files in os.walk(data_folder):  \n",
    "    for file in files:\n",
    "        if file.startswith(\"classement_f1_\") and file.endswith(\".csv\"):\n",
    "            file_path = os.path.join(root, file)\n",
    "            year = extract_year_from_filename(file)\n",
    "            classement_files[year] = pd.read_csv(file_path)\n",
    "        if file.startswith(\"calendrier_f1_\") and file.endswith(\".csv\"):\n",
    "            file_path = os.path.join(root, file)\n",
    "            year = extract_year_from_filename(file)\n",
    "            classement_files[year] = pd.read_csv(file_path)\n",
    "        if file.startswith(\"circuits_f1_\") and file.endswith(\".csv\"):\n",
    "            file_path = os.path.join(root, file)\n",
    "            year = extract_year_from_filename(file)\n",
    "            classement_files[year] = pd.read_csv(file_path)\n",
    "        if file.startswith(\"constructeurs_f1_\") and file.endswith(\".csv\"):\n",
    "            file_path = os.path.join(root, file)\n",
    "            year = extract_year_from_filename(file)\n",
    "            classement_files[year] = pd.read_csv(file_path)\n",
    "\n",
    "# ğŸ“Œ VÃ©rifier que les fichiers sont bien chargÃ©s\n",
    "for year, df in classement_files.items():\n",
    "    print(f\"ğŸ“‚ ChargÃ© : classement F1 {year} â†’ {df.shape[0]} lignes, {df.shape[1]} colonnes\")\n",
    "\n",
    "\n",
    "def search_f1_data(question):\n",
    "    question = question.lower()\n",
    "    search_result = None\n",
    "    files_to_return = []  # Liste des fichiers CSV Ã  retourner\n",
    "\n",
    "    # ğŸ“Œ VÃ©rifier s'il y a une annÃ©e spÃ©cifique dans la question\n",
    "    match = re.search(r\"(20\\d{2})\", question)\n",
    "    year = int(match.group(1)) if match else None\n",
    "    \n",
    "\n",
    "    # ğŸ“Œ VÃ©rifier si l'utilisateur demande plusieurs annÃ©es\n",
    "    if any(keyword in question for keyword in [\"5 derniÃ¨res annÃ©es\", \"derniÃ¨res annÃ©es\", \"cinq ans\"]):\n",
    "        if year:\n",
    "            start_year = max(2020, year - 4)  # Assurer qu'on ne descend pas sous 2020\n",
    "        else:\n",
    "            start_year = 2020  # Par dÃ©faut, on prend les 5 derniÃ¨res annÃ©es disponibles\n",
    "        year = list(range(start_year, start_year + 5))  # GÃ©nÃ¨re les 5 annÃ©es demandÃ©es\n",
    "    else:\n",
    "        years = [year] if year else []  # Sinon, utilise l'annÃ©e trouvÃ©e\n",
    "    print(year)\n",
    "    # ğŸ“Œ VÃ©rifier si la question concerne le classement\n",
    "    if \"classement\" in question:\n",
    "        files_to_return = [f\"classement_f1_{y}.csv\" for y in year]\n",
    "        print(files_to_return)\n",
    "\n",
    "    # ğŸ“Œ VÃ©rifier si la question concerne le calendrier des courses\n",
    "    elif \"calendrier\" in question:\n",
    "        files_to_return = [f\"calendrier_f1_{y}.csv\" for y in year]\n",
    "\n",
    "    # ğŸ“Œ VÃ©rifier si la question concerne les circuits\n",
    "    elif \"circuit\" in question:\n",
    "        files_to_return = [f\"circuits_f1_{y}.csv\" for y in year]\n",
    "\n",
    "    # ğŸ“Œ VÃ©rifier si la question concerne les constructeurs\n",
    "    elif \"constructeur\" in question:\n",
    "        files_to_return = [f\"constructeurs_f1_{y}.csv\" for y in year]\n",
    "\n",
    "    # ğŸ“Œ Charger et concatÃ©ner les fichiers trouvÃ©s\n",
    "    data_results = []\n",
    "    for file_key in files_to_return:\n",
    "        file_path = os.path.join(data_folder, file_key)\n",
    "        if os.path.exists(file_path):\n",
    "            df = pd.read_csv(file_path)\n",
    "            data_results.append(f\"ğŸ“‚ **DonnÃ©es de {file_key}**\\n{df.to_string(index=False)}\\n\\n\")\n",
    "\n",
    "    # ğŸ“Œ Retourner les rÃ©sultats concatÃ©nÃ©s\n",
    "    if data_results:\n",
    "        return \"\\n\".join(data_results)\n",
    "\n",
    "    return \"DÃ©solÃ©, je n'ai pas trouvÃ© de donnÃ©es correspondantes.\"\n",
    "\n",
    "\n",
    "# ğŸ“Œ **GÃ©nÃ©rer une rÃ©ponse avec le LLM**\n",
    "def generate_response(question, data_results):\n",
    "    prompt = f\"\"\"\n",
    "    [INST] \n",
    "    Tu es un expert en Formule 1. Un utilisateur te pose la question :\n",
    "    \"{question}\"\n",
    "\n",
    "    Voici les donnÃ©es brutes extraites des fichiers CSV :\n",
    "    {data_results}\n",
    "\n",
    "    Analyse les donnÃ©es et donne une rÃ©ponse claire et concise.\n",
    "    [/INST]\n",
    "    \"\"\"\n",
    "\n",
    "    output = llm(prompt, max_tokens=256)\n",
    "    return output[\"choices\"][0][\"text\"]\n",
    "\n",
    "\n",
    "\n",
    "# ğŸ“Œ **Lancer le chatbot interactif**\n",
    "while True:\n",
    "    user_input = input(\"Pose une question sur la F1 : \")\n",
    "    if user_input.lower() == \"exit\":\n",
    "        break\n",
    "\n",
    "    # ğŸ” Recherche dans les fichiers CSV\n",
    "    data_results = search_f1_data(user_input)\n",
    "\n",
    "    # ğŸ§  GÃ©nÃ©ration de la rÃ©ponse avec le LLM\n",
    "    response = generate_response(user_input, data_results)\n",
    "    print(\"F1Bot:\", response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
